<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, posted on Brightspace.  

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<!--<base href="https://engineering.purdue.edu/ece477/StudentWebTemplate/" />-->
    <base href="https://engineering.purdue.edu/477grp16/" /> <!-- Replace the N with your team number-->
<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
            <h2>Progress Report for Joey Collins</h2>

            <h4>Weeks 14-15:</h4>
            <b>Date:</b> 12/01/2023<br>
            <b>Total Hours:</b> 40<br>
            <b>Cumulative Hours:</b> 154<br>
            <b>Description of Project Design Efforts:</b><br>
            These past 2 weeks were spent on finalizing the project. In addition to a significant amount of time that went into integration, there was also an unexpectedly large amount of debugging that presented itself the week of the demo.<br><br>

            <b>Robot Logic</b><br>
            In order to implement the code for the robot to defend the goal, a bit of planning had to go into the way it would move. Several methods were thought of and discussed, but I eventually landed on a system that split the table into two sections. The user side of the table is inaccessible to the robot due to physical limitations and game rules. 
            In order to combat the lack of things the robot can do while the user plays with the puck on their side, I wrote code that will "shadow" the puck's movements on the user side of the table while staying as far back as possible. This also prevented the mallet from ramming into the motors. Once the puck crossed from the user side to the robot side, the mallet was free to move towards the puck.<br><br>

            <b>Shadowing and Puck Interception</b><br>
            After working with direction detection last week, I was confident in adding some extra functionality to the system. I decided that in order to lessen the distance that the robot needs to travel to intercept the puck, it should "shadow" the puck's movements on the user side of the table while staying as far back as possible. 
            This functionality turned out to be relatively easy to implement. It also seemed to improve the ability of the robot to reach the puck with the current motor speeds. Once shadowing was implemented, I worked on code to intercept the puck on the robot side of the table. This checked for the current location of the puck and gave the motors one of the following instructions: {up, down, right, left, ur, ul, dr, dl}. The logic for this is accurate, but the accuracy in gameplay is completely dependent on the abilities of the motors.<br><br>
            
            <b>Motor Speed Adjustments</b><br>
            One of the problems that presented itself while testing this game code was the stalling and/or failure of the motors. Previously, we had run the motors from 0 to max speed instantly, but found that this rapid change in speed could stall the motors and would render them useless. In order to combat this, Abby and I worked on code that accelerated and decelerated the motors using a timer on the microcontroller. This made the stalling less frequent, but did not completely solve the issue. We believe that this issue could be a mechanical and/or electrical issue. In order to treat the problem for demonstration purposes, we slowed down the motors.<br><br>

            <b>Miscellaneous Debugging</b><br>
            Abby and I spent a significant amount of time the week of the demo trying to solve random issues that did not present themselves until the last minute. Most of the time went towards fine-tuning the settings on the motors and addressing some errors with the 7-segment displays. They were displaying the wrong digits at some points in time, so we had to sweep through the micro code to find a small bug that created that problem. 
            We also had issues with the clock used for SPI on the 7-segments affecting the motors. We had to flash our 7-segments at a lower rate in order to ensure the motors were being updated with instructions as quickly as they were received from the PC doing the visual analysis. Finally, the red logo on the table was causing some problems with the color detection of the mallet, so I went ahead and spray painted it white.<br><br>

            <b>Demonstration</b><br>
            The final part of these two weeks was the demonstration. We were able to get all of our PSDRs checked off and were very proud to have accomplished what we did during this semester. I learned a lot of things that I would never be able to gain from other types of classes because of the amount of hands-on work I did. I would like to thank Phil, Joe, and Andjey for all of the help that we received this semester and for being great mentors. Shown below is the final state of the table as shown in the demonstration!<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\userFinal.mp4" type="video/mp4"></video><br>
            <i>User POV</i><br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\robotFinal.mp4" type="video/mp4"></video><br>
            <i>Camera POV</i><br><br>


            <h4>Week 13:</h4>
            <b>Date:</b> 11/17/2023<br>
            <b>Total Hours:</b> 15<br>
            <b>Cumulative Hours:</b> 114<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I was able to begin work on integrating the computer vision software into our physical design. This consisted of doing some calibration, but for the most part I 
            worked on methods to slowly introduce new functionality without the possibility of destroying the table. There is a great level of caution required as we do not have limit switches mounted.<br><br>

            <b>Reformatting Code</b><br>
            In order to prepare for the moment when integration could finally begin, I worked to rewrite the computer vision software to be more modular. This was done to have more control over how 
            I am executing the program to ease the debugging process. I should now have more flexibility with how processes are done, so I can test individual components of the algorithm.<br><br>

            <b>Distortion Removal</b><br>
            This week, I was able to test how effective the radial distortion removal was in live video. With the test photos, it was quite clear that the curved lines were straightened out to allow for more accurate depictions of the table. 
            Unfortunately, the distortion removal behaved as I expected with live video. As each frame had to be individually processed, quite a bit of latency was introduced. With this discovery, the team has decided to sacrfice accuracy for speed and remove the distortion removal functionality.<br><br>

            <b>Direction Detection</b><br>
            In order to begin work on telling our motors how to move in order to reach a point, I wrote some code that took the position of the puck and the position of the mallet into account. I recently wrote the code that determines the position of the mallet in the same fashion that the position of the puck is determined. We have taped a temporary post-it note onto the top of the gantry, 
            but we plan on placing a permanent purple sticker on the top, as there exist no similar colors on the table. Shown below is a video of the direction detection software. The terminal prints the direction. The output "ur" stands for up and right, "dr" stands for down and right, "ul" stands for up and left, and "dr" stands for down and right.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\direction.mp4" type="video/mp4"></video><br>
            <i>Direction Detection</i><br>

            Once this was completed, I worked with Abby to convert this instructions into motor movement. She wrote good code that serialized these instructions and sent them to the corresponding motors. Shown below 
            is a video of the mallet and puck constantly being tracked while the mallet attempts to reach the location of the puck. We are moving the motors at a slow speed as we are just testing and did not want to break the table.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\primitiveTrack.mp4" type="video/mp4"></video><br>
            <i>Puck Following</i><br>

            Next week, I will continue working on getting the speeds up and determining exaclty what point I want to send to the motors. I have not yet determined the optimal location to send and how often to send it. I will also need to make software limit switches to ensure that the mallet does not 
            ram into the walls of the table. I will be working closely with Abby to get the robot functional in the near future.<br><br>
            



            <h4>Week 12:</h4>
            <b>Date:</b> 11/10/2023<br>
            <b>Total Hours:</b> 12<br>
            <b>Cumulative Hours:</b> 99<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I spent some time soldering some ICs on our PCB. After I moved on from PCB assembly, I began some preliminary work on getting rid of the radial distortion in our camera. 
            In addition to physical work on the project, I also participated in the Moonshot Pitch challenge to try to secure more funding for the team.<br><br>

            <b>PCB Assembly</b><br>
            This past weekend, Abby and I spent several hours in the lab preparing for solder components onto our PCB. We decided that it would be best if she did the discrete components and I focused on the various chips. 
            Eventually, we decided that we were ready to begin working on our final PCB. Abby started off by doing some of the decoupling caps for the power to the microcontroller and various other resistors and capacitors. After testing our power subsystem, we decided to move on to the micro and the UART-to-TTL chip. 
            I started off with the UART chip. I was able to get all of the pins on the pads by using the drag soldering technique we learned in lecture. Unfortunately, a couple of bridges were formed during the process. I tried removing them by sucking the solder up with the iron by dragging it from the top of the pin to the bottom. However, 
            I ended up burning off several of the pads on the board. This was quite frustrating, as Cameron and Abby had already spent so much time assembling other components on the board and we would have to start over. Abby and I instantly started work on our second board. This time, I had more luck with soldering the UART-to-TTL and the microcontroller. I did leave a couple of bridges, but I did not attempt to remove them this time, 
            as I clearly did not do very well on my previous attempt. Shown below is an image of the second PCB after I left the bridges.<br><br>

            <img src="Team\progress\colli384_files\pcb.jpg" width="530" height="310"><br>
            <i>Unfinished PCB</i><br><br>
                   
            Cameron was nice enough to help me out and remove those. The microcontroller ended up working after doing our systematic testing, which was a relief. We did have some problems getting the UART to work, but we had issues. The team had overlooked an issue with the way the PCB was designed, which meant that we had to flywire the UART-to-TTL board we used in 362. Luckily, this worked well and we have established successful UART communication between a computer and the microcontroller.<br><br>

            <b>Distortion Alleviation</b><br>
            This week I spent some time looking into a method to remove some of the radial distortion intrinsic to the camera. Luckily, <a href="https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.html#calibration">OpenCV</a> has methods for fixing problems like this, so I did not have to consult other external libraries. Their recommendation includes 
            the calibration of the camera by taking several control pictures that have a standard chessboard in the frame. I printed out this chessboard and wrote the code to make use of the calibration photos. The whole idea behind this method is that OpenCV knows how large each of the squares on the board are and 
            how far apart they are supposed to be. Using this information, they can use several different pictures with the chessboard displayed at different angles to determine a matrix that represents the distortion caused by the camera. It can then use this matrix to invert the distortion and create a clean image without the distortion. Shown below is a video of the code processing several control images and then displaying an original image and a fixed image.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\undistortion.mp4" type="video/mp4"></video><br>
            <i>Distortion Removal</i><br><br>

            There are several problems that could arise from the use of distortion removal in this project. The first of which is the fact that when radial distortion is removed, the field of view is sacrificed. This means that we would have to mount the camera even higher to get the same field of view that we had before removing distortion. Shown below are two images. The first of which is the distorted image and the second is the fixed image. It is clear to see that the fixed image has been cut off in several locations.<br><br>

            <img src="Team\progress\colli384_files\WIN_20231109_14_37_07_Pro.jpg" width="530" height="310"><br>
            <i>Original Image</i><br><br>

            <img src="Team\progress\colli384_files\calibresult.png" width="530" height="310"><br>
            <i>Fixed Image</i><br><br>

            The second problem that could potentially arise is the fact that for live video, each frame would have to be fixed before calculations could be done on puck location. I am not currently sure about how much latency this would introduce. After this value is determined, the team will have to decide on what has priority: speed or accuracy. Hopefully, the introduced latency is not of a massive magnitude and no sacrifices will have to be made.<br><br>

            <b>Moonshot Pitch Challenge</b><br>
            As mentioned last week, the team was fortunate enough to be selected to participate in the finals of the <a href="https://purdueincubator.org/moonshot-pitch-challenge/">Purdue Innovates Moonshot Pitch Challenge</a>. Abby prepared an excellent script and slides for us to pitch to the panel of judges. We were placed in the social category of this competition, as our table was advertised as a product that could teach students about STEM. Our team was eligible for 3 different rewards: best social pitch, best overall pitch, and crowd favorite. These rewards had values of $1500, $500, and $500, respectively. Unfortunately, we were not successful in winning in any of these categories, so we went home with no extra funding for the table.<br><br>

            This weekend, the table should be fully constructed from a mechanical standpoint, so a lot of new testing will be able to commence. I am quite looking forward to integrating the components we have been working on.<br><br>



            <h4>Week 11:</h4>
            <b>Date:</b> 11/03/2023<br>
            <b>Total Hours:</b> 9<br>
            <b>Cumulative Hours:</b> 87<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I worked on components of the project related to PCB assembly. Most notably, I spent a large chunk of time practicing my soldering technique. 
            The PCB assembly was started, but will be finished this weekend. In addition to assembly, I also started looking into getting rid of the radial effect that our camera creates at large distances from the table. 
            This may not be a major concern depending on how drastic the radial effect is. This will likely be something that is addressed once we start prototyping. Finally, this week I attended a workshop for the finals of the Moonshot pitch competition.<br><br>

            <b>PCB Assembly</b><br>
            Abby and I spent time this week continuing to practice the techniques necessary to assemble our PCB. Most of the practice I did was with the drag soldering technique used on components like our microcontroller and various ICs. 
            I used the practice boards and empty QFP components to do this. I found that I was able to get solder on each of the pins, but would form bridges at the edge that I started at most times. I think this might be due to the amount of time/pressure I apply 
            when I start, but I am not completely sure. I got better at removing these bridges by taking the knife edge of the iron and "dragging" it from the top of the pin to the bottom, away from the chip. This seems to work often, even if it does take multiple tries. I also practiced 
            removing solder using the desoldering material. Abby and I also practiced with some discrete components like resistors and capacitors. For some reason, I found these to be more difficult than components that I can use drag soldering on. I had problems with either 
            using too much solder or too little. Abby was quite good at doing these, however. We decided that it would probably be best for me to do chips and for her to do discrete components. On 11/3, Cameron assembled and tested the power-related components, so Abby and I will be assembling the micro-related components on 11/4.<br><br>

            <b>Radial Distortion</b><br>
            This week, we were able to finish designing and printing the mount that holds the camera above the table. Once we placed the camera in the mount, we realized that there were a couple of problems. We incorrectly estimated the height that we would need to raise the scoreboard 
            to view the entirety of the table. We will have to reprint the height extenders that we have been using. The second problem we noticed was a significant amount of radial distortion as the camera is moved farther away from the object of interest. Shown below is an image depicting the radial effect.<br><br>

            <img src="Team\progress\colli384_files\fisheye.png" width="530" height="310"><br>
            <i>Radial Distortion</i><br><br>

            Unfortunately, this distortion could potentially cause problems because the tracking algorithm I developed assumes that the walls of the air hockey table are straight lines. This is clearly not the case once radial distortion is introduced. This weekend, I will test to see how big of an effect 
            the distortion has on the predicted path of the puck. If there is not a major impact on accuracy, then I will not focus on this problem at the moment. However, if there is a major impact, I will begin fixing it. I have done some preliminary research into methods that 
            OpenCV recommends for fixing radial distortion. <a href="https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_calib3d/py_calibration/py_calibration.html#calibration">This tutorial</a> seems to be quite helpful for this. It appears that the camera should be calibrated before each use, in order to make sure that the distortion matrices are accurate for each session. This could be done 
            in a similar fashion to how the color calibration is done. The chess board could be part of the initial setup, or even drawn straight onto the table.<br><br>

            <b>Moonshot Pitch Preparation</b><br>
            Recently, the team applied to compete in the <a href="https://purdueincubator.org/moonshot-pitch-challenge/">Purdue Innovates Moonshot Pitch Challenge</a>. This is an ideation-focused competition that invites pitches for concepts that will solve real-world problems. Abby was quite creative and marketed our table as a way to 
            educate young students on electrical and computer engineering topics. We were fortunate enough to be named a finalist in the social category for the competition, which means that we get to do a live pitch next week. Purdue Innovates was kind enough to host a workshop to help the participants prepare for the event. Abby and I spent around 2 hours at this workshop and were able to get some good 
            information. Hopefully next week we will be able to write about a winning pitch in the progress report. Next week, we plan on having the entire gantry system in place. This is a crucial step in our prototyping progress, as we should have the PCB done as well. This is when the integration process really begins. I am excited to start getting meaningful information sent to the motors and seeing some movement on the table.<br><br>



            <h4>Week 10:</h4>
            <b>Date:</b> 10/27/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 78<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I worked on a lot of small components of the project. I continued work on the smoothing of the data being sent to the microcontroller and also fixed some bugs that 
            were found during the testing of color calibration using live video. I also practiced soldering components onto a PCB as Abby and I will be the ones assembling the PCB for the team.<br><br>

            <b>Data Smoothing</b><br>
            Last week, it was mentioned that it seemed like the computer vision software might be sending too much data too frequently. This was quite apparent when we ran some test code on old motors. The old motors did not have a frequency 
            high enough to handle the rapid requests being made. In order to fix this problem, I have once again tried working more on sending less data. I am struggling to think of more ways to do this without losing accuracy. This week, I worked more with doing a version of boxcar 
            averaging the data to be sent so the requested locations are closer together. However, this seems dangerous to me because if we lose the puck and it appears somewhere else very quickly, then the boxcar averaging will take several frames to catch up to this location because 
            it is averaging it with several other old locations nowhere near the new location. I think this is something that will have to be tested using the actual gantry system in the future in order to more accurately assess the issue. I also think that because the table is so long, we should have more 
            time than we have in our tests to determine the final location, which could solve some of the problems.<br><br>
        
            <b>Color Calibration Bug Fixes</b><br>
            Last week, I worked on creating a system that will automatically generate a color range for the puck no matter what lighting it is in. To do this, it analyzes the pixels in the middle of the initial 
            frame and determines an optimal range that those colors fit into. During the creation of this algorithm, I did my testing with a pre-recorded video. When I came into lab to test this software with live video, 
            it failed. Initially, I was very confused as to why the software was failing to generate a color range that the puck fell into. However, after spending an embarrassing amount of time troubleshooting, I was able to solve the problem. The fix ended up relating to how OpenCV 
            treats the HSV scale. <a href="https://cvexplained.wordpress.com/2020/04/28/color-detection-hsv/#:~:text=In%20OpenCV%2C%20Hue%20has%20values,255%2C%200%2D255).">These notes</a> did a good job of explaining the HSV scale in OpenCV. I am still not quite sure why the original algorithm I used 
            worked on pre-recorded video and not live video. The live video is the important aspect, so I am content with how the color calibration software is now. Shown below is a demonstration of the color calibration. In the first half, the initial frame has green, so the green puck can be tracked. In 
            the second half of the video, the initial frame is a blue puck, so it tracks the blue puck.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\colorCalibration.mp4" type="video/mp4"></video><br>
            <i>Color Calibration</i><br><br>
            In the section of the video where the blue puck is being tracked, it can be seen that the blue area on the table is momentarily tracked. This would be a problem if we were to be using a blue puck. However, we chose the color green for the puck 
            for this reason exactly. We should not have any instances where two objects of the same color are present on the table.<br><br>

            <b>Soldering Practice</b><br>
            Prior to this week, I had only soldered once in my entire life. This week, I focused on improving my skills in this area because I do not want to mess up any of our components or our PCB. Joe was nice enough to 
            give me and Abby a very helpful tutorial in soldering quad flat pack components and also in removing solder. We practiced ourselves and the first couple of attempts were not very pretty. However, we did learn what not to do in some situations. We also learned 
            that when in doubt, use more flux! I am still trying to figure out how quickly and how much pressure I should apply when using the drag technique. I am also not very good at fixing any bridges that I make in the process of soldering. I am going to continue practicing before I 
            move on to assembling our actual PCB.<br><br>

            Next week, the goal is to have the gantry system mostly assembled upon the table. I hope to begin working with some motor control to determine any kind of limits I should place upon the software. For example, I want to implement some safety features where 
            if the computer realizes that it is about to send the system beyond where it can physically reach, it cancels that instruction. I think next week will be lots of integration and PCB assembly.<br><br>




            <h4>Weeks 8-9:</h4>
            <b>Date:</b> 10/20/2023<br>
            <b>Total Hours:</b> 14<br>
            <b>Cumulative Hours:</b> 70<br>
            <b>Description of Project Design Efforts:</b><br>
            In week 8, a significant amount of time was dedicated to the Midterm Design Review Presentation. In week 9, I focused on working more on software. Specifically, I worked on some serial communication foundations with Abby and I developed a way to calibrate the color range for the 
            puck in the scenario that the lighting or puck color changes.<br><br>

            <b>Midterm Design Review Presentation</b><br>
            As stated previously, the majority of week 8 was dedicated to the creation and fine-tuning of the presentation. I was in charge of the software development status section and the
            project timeline section. For the software development status section, I created two charts to demonstrate the individual components of the software. The first chart covered the software that will be running on the PC. This included sub-components for visual analysis and data transfer. The other chart 
            was dedicated to the software that will be running on the microcontroller. This chart included sub-components for output control, PC interfacing, and sensor interfacing. I also took the liberty of including videos to give a visual of the computer vision software that I have been working on. I think this helped give the reviewers a better understanding 
            of what I was explaining. The other section that I covered was the project timeline information. This was not a very difficult task, as everyone is quite aware of the strict timeline that we are currently working with. I decided to use a Gantt chart to represent the components that we are working on and when we hope to have them completed. Overall, the presentation went fairly well. I think that we struggled 
            a little bit with timing, but I am not too upset with that as we had a lot of information to cover with a project this involved.<br><br>

            <b>Puck Dependent Motor Manipulation</b><br>
            One of the more exciting components of the project that I worked on in week 9 dealt with controlling the motors based on information coming from the computer vision software. To do this, 
            I worked closely with Abby and her data serialization software. This software is able to take data and convert it into a format that can be sent and received via UART. This is how we will be sending the data from the PC to the microcontroller. 
            In order to begin working on this basic integration, I altered the software to provide a displacement. Every time the motor is requested to be at a new location, that location is cached. Upon the next request, the distance between the new request and the previous request is calculated and returned. This is the displacement that is being used in this example. 
            Once a displacement is found, the data is serialized and sent to the microcontroller. To check to make sure the data can be translated into instructions for the motors, Abby and I decided to manipulate the direction of the motor spin based upon the sign of the displacement. For example, 
            a positive displacement would result in clockwise spin, and a negative displacement would result in counter-clockwise spin. We did not work with any varying speeds or steps because at this point we only wanted to ensure that we were able to get data from the PC to the motors with no problems. Shown below is a video demonstrating this testing.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\ahaluska_files\IMG_1442.mp4" type="video/mp4"></video><br>
            <i>Motor Control using Puck Data</i><br><br>

            One of the disappointing parts of this testing was finding that the motor could not switch direction as frequently as the data was being sent. We were working with an older motor because our new motors were in the machine shop at that time. The hope is that the new motors are able to handle these frequencies better. If not, I will have to work more on smoothing out the detections 
            and being more careful with how often I update the preferred mallet location. Going forward, I will keep this in mind as I continue writing code for the motors.<br><br>

            <b>Color Calibration</b><br>
            The other problem that I concentrated on this week was finding a way to determine the puck color without hard coding it in prior to runtime. This was necessary because of the fact that 
            lighting plays such a large role in how the camera picks up the color of the puck. If the table were to be in a different location than the lab, the range for green that we had established would be completely useless. For example, 
            when the table is transported to MSEE for the SPARK challenge, the lighting would be completely different and the puck would most likely not be found in the range that had been determined previously. To combat this, 
            I have written a script that locates the center of the table (where the puck should be placed during initialization) and calculates an appropriate HSV color range for the color that appears in the middle of the table. I originally thought that I may have to implement some sort of color correction like the method found on <a href="https://pyimagesearch.com/2021/02/15/automatic-color-correction-with-opencv-and-python/">pyimagesearch</a>, but I did not have a persistent need for color correction. I 
            just needed a one-time calculation for a color range.<br><br>
            Looking forward, I would like to continue fine-tuning the data that we are sending out so hopefully we can send data less frequently. This will need to be done in conjunction with the implementation of mallet tracking as well, which I hope to start very soon.



            <br><br>

            <h4>Week 7:</h4>
            <b>Date:</b> 10/06/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 56<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I made the switch to live video analysis. Most of my time was spent fixing bugs that manifested due to the switch between the pre-recorded video and live video.<br><br>

            <b>Color Matching</b><br>
            One of the seemingly simple tasks this week ended up being on of the more time consuming tasks for me. In the software that I had written for the pre-recorded video, the puck is found using color detection. I had to set a range of color in the Hue, Saturation, Value (HSV) scale that the puck could be found in. This was quite easy because the puck 
            was a vibrant red color and the lighting in the video was fairly constant so it did not change appearance much. However, after making the switch to live video I found that finding the range for our green puck to be quite difficult. Part of the problem came from my lack of understanding of how the HSV color scale actually works, so some of the testing was just guessing 
            and checking. I also tried making use of a <a href="https://redketchup.io/color-picker">color picker</a> online. However, this did not work as well as I would have like as the software was still losing the puck quite frequently. I used the value that I got from the color picking website as a starting point and continued my guess and check method. It took several hours of testing until I was able to find a range that 
            consistently matched the puck. Shown below is an image of the green puck being detected in the lab. <br><br>

            <img src="Team\progress\colli384_files\live pic.png" width="530" height="310"><br>
            <i>Green Puck Detection</i><br><br>

            <b>Adjustments for Live Video</b><br>
            After I was able to find the correct color range to find the puck, I decided to run my software that I developed for the pre-recorded video without changes on the live feed. Before I could do this, I had to mount the camera to the top of the table. To do this, I reattached the scoreboard that came with the table. This scoreboard conveniently sits right above the center of 
            the table, so I was able to leave it there and connect the camera to it using zip ties. Unfortunately, I have the camera 90 degrees from the position it needs to be in to view the whole table, so I was working with a weird angle this week. Once the camera was in place, I went ahead and ran my software with the new color range. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\initial live.mp4" type="video/mp4"></video><br>
            <i>Live Video with Initial Parameters</i><br>

            I knew prior to this initial testing that the output would be far from what we wanted. The main problem that I needed to account for was the position of the walls within the video. Like I have said previously, I want to put certain colored stickers on the corners of the table that will indicate 
            the positions and update over time in case the table moves between frames. For now, I am hard-coding the position of the walls in the new video. Due to the camera being oriented incorrectly, I also had to improvise and assume that the right side of the frame was the goal, but it was actually just a wall. This is fine for now, but 
            I will have to adjust the camera soon to start working with the real situation. After fixing the wall locations, I started testing again. This time, the intersection locations of the puck seemed to be much more accurate and I was able to actually see the predictions being made, which were correct for the most part. However, it seemed to twitch more than I would 
            like, so I adjusted some of the parameters for the number of frames in the history of previous intersections and the pixel tolerance for notifying about a new instance of the potential for a goal. This seemed to clean up some of the noise. Shown below is some live feed that was captured to demonstrate the software working with the right side of the table acting as the robot goal.<br><br>

            
            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\Live Feed.mp4" type="video/mp4"></video><br>
            <i>Live Video with Updated Parameters</i><br>

            I was quite happy to see that the transition from pre-recorded video to live video was not as horrific as I had thought. There is still a lot of cleaning up that needs to be done to allow for smooth motor movement, but I think the software is on the right track. 
            This week I learned that we will definitely need to spend time perfecting the parameters and probably add some more algorithms that can lessen the amount of times the motors have to move the robot mallet. Looking forward, I would like to get the stickers to mark the end of the table and I would also like to 
            get some kind of sticker that will indicate where exactly the mallet is located on the table so I can start to track both mallet and puck position at the same time. This will be useful because I will be able to send the microcontroller information about exactly the mallet needs to move, not where it needs to move to. This will decrease 
            the amount of processing that the microcontroller needs to do, allowing for lower latency.<br><br>


            <h4>Week 6:</h4>
            <b>Date:</b> 09/29/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 48<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I started making additions to the puck tracking software with the motor application in mind. I wanted to make sure that the motor is not constantly twitching due to non-essential puck data. I also started developing a plan for how the coordinate system 
            on the table should be executed.<br><br>

            <b>Smoothing Predictions</b><br>
            I noticed that the predictions last week were quite accurate, but the fact that the puck coordinates were constantly jumping around would put quite a strain on the motors. I tried to address this issue by implementing some averaging on the locations where the puck might cross into the "danger zone". 
            To do this, a history of previous locations where the puck crossed into that zone is kept. The length of this history can be changed, but for now I have it keeping the 3 most recent intersections. Using this history, I average out the 3 previous intersection points and output one singular point that takes advantage of the previous data. This should 
            provide more accurate predictions because of the fact that it is no longer relying on just two consecutive frames. It should also decrease the likelihood of the motor making a sporadic movement due to some outlying frame. Keeping the motors in mind, I also created a sort of tolerance to ensure that the micro is not being sent useless data. Essentially, 
            the last location sent is compared to the new location waiting to be sent. If the two differ by more than the tolerance amount, then the new location will be sent. Otherwise, it will not be sent and the motor will stay where it is. This is to ensure that the motors are not being sent information that will result in unnecessary movement. I am not totally sure that these additions will be completely necessary in the final product. 
            It could turn out that the motors can handle extra movement completely fine, and it is preferable to have no averaging. I implemented this functionality just so that we have it in case we need it. It also allows for some customization later on. Shown below is a demonstration of the averaging being used with a history of 3 frames and a tolerance of 30 pixels. <br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\point averaging.mp4" type="video/mp4"></video><br>
            <i>Puck Location Averaging</i><br><br>

            <b>Grid Planning</b><br>
            The next piece that I worked on this week was developing a plan for how the table will be divided into coordinates for the purpose of moving the robot mallet. The ideal solution would be using the pixel values for the coordinate system. 
            I believe that this would not be easy to execute because of the number of pixels in the image, requiring a large amount of precision in the motor movement. Instead, I think it would be beneficial to scale the pixel values down into smaller values. For example, the top of the table in 
            the pre-recorded video is at pixel value 90. The bottom of the table is at pixel value 625. If we let the top of the table be 0 and let the bottom of the table be 25, then we can use the equation new y-coordinate = 0.0467(old y-coordinate) - 4.21, then we can translate the pixel value into a coordinate system that goes from 0-15. The same can be done for the x-direction. 
            I think that using a coordinate system like this would also decrease the amount of twitching in the motors. For example, a puck prediction that varies by several pixels might cause a jerk in the motor, but several pixels translated to a grid system with fewer discrete points would not make a difference and the motor would not necessarily have to move. 
            I think that a grid with more than 0-15 would be necessary, but that was an easy example to use. This week I learned that it might be more difficult than I thought it would be 
            to optimize the puck detection process. I may have to spend many hours in the future doing some calibration once we move to a live feed.<br><br>
            
            I did not do any testing with live video this week because work is still being done on the table for the mechanical components. I hope that next week, we will be able to mount the camera at the top of the table to start testing live video. If this is not possible, 
            I plan to begin developing a plan on the exact data to send to the microcontroller to move the motors. Unfortunately, this will require knowledge of where the robot mallet is located. In live feed, I was planning on doing this using color differentiation, but in the pre-recorded video the puck and mallets are the same color. I would have to write the code without being able to test it.<br><br>


            <h4>Week 5:</h4>
            <b>Date:</b> 09/22/2023<br>
            <b>Total Hours:</b> 9<br>
            <b>Cumulative Hours:</b> 40<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I continued working on the tracking software. The components I worked on this week were the ability to predict future bounces on the table and a system to detect when and where the puck will potentially cross some threshold that indicates potential for a goal.<br><br>
            <b>Future Bounce Prediction</b><br>
            As of last week, the tracking software was able to determine where a puck might hit a wall based on its current trajectory. This week, I wanted to expand on that capability by predicting where the puck might go after that bounce. 
            In order to realize this goal, I used some knowledge of geometry. Shown below is the formula for finding the <a href="https://www.onlinemathlearning.com/anglebetweentwolines.html">angle of intersection</a> between two infinite lines using slope.<br><br>
            <img src="Team\progress\colli384_files\angle.png" width="220" height="115"><br>
            <i>Angle of Intersection Formula</i><br><br>
            
            After observing this formula, it can be shown that a line intersecting another line with an undefined or 0 slope will result in a reflection with the negative of the original slope. 
            Once this was determined, I could use the same function I had been using for bounce detection, but this time with the new reflection line. I would also have to update the velocity so the function knew which way the puck was traveling along that line. This was simple to do because if the puck bounces off of the top or bottom walls, then the x component stays the same and the y component is negated. The opposite is true for bounces off of the left or right walls. 
            I found that I could recursively use this function to predict as many future bounces as I want. I have the number currently capped at 3 for observational purposes. This seems like a good number to use for testing as well because pucks appear to slow down significantly after 3 bounces. 
            Shown below is video demonstrating the bounce prediction functionality. The three red dots indicate the 3 places where the puck will strike if no other external forces act upon it.<br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\future bounce.mp4" type="video/mp4"></video><br>
            <i>Future Bounce Prediction</i><br><br>
            <b>Goal Potential Detection</b><br>
            The next component that I worked on was trying to determine when the puck has a chance of coming near the robot's goal. The current method that I have for doing this is establishing a line segment slightly in front of the physical goal that represents a "danger zone". 
            Currently, this line segment goes from the top of the table to the bottom of the table and rests about 100 pixels in front of the goal. I edited the function that predicts future bounces to include the software that checks if the current path or any reflections will cross that line into the "danger zone". 
            It also checks to make sure the puck is moving towards the robot goal before indicating that there is potential for a goal. In the future, I think this will be an important function to prevent the motors from having to twitch at every movement of the puck and instead wait until there is an apparent scoring chance. 
            Shown below is video demonstrating this functionality. Any time there is a red line segment with a green dot at the end of it, the puck is predicted to cross into this "danger zone" within 3 collisions of its current path. The red line segment shows what bounce the scoring chance will come from.<br><br>
            
            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\danger zone.mp4" type="video/mp4"></video><br>
            <i>Goal Scoring Potential Prediction</i><br><br>

            I think that good progress was made this week in terms of tracking software development. I would really like to begin working with live video as soon as we can get the camera mounted above our table. I believe that my current code can be easily edited to theoretically work with live video, 
            but sometimes things do not work that way. Next week, I hope to begin optimizing my current code so the predictions do not jump around as much. I want to do this because I want to prevent unnecessary motor movement.<br><br>


            <h4>Week 4:</h4>
            <b>Date:</b> 09/15/2023<br>
            <b>Total Hours:</b> 7<br>
            <b>Cumulative Hours:</b> 31<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I started working on how the tracking software should be making predictions on final puck location. Specifically, I worked on handling collisions with walls. Unfortunately because of Industrial Roundtable, I was not able to progress as far as I wanted to this week.
            <br><br>
            <b>Table Detector</b><br>
            In order to determine when a puck is going to collide with a wall, the software needs to know where the walls are located. To do this, I want to locate the corners and determine the edges using those points. My initial idea for doing this was using the OpenCV object detector. I wrote some code to detect any objects with 4 edges in the video, but occlusion became quite a significant problem in this method. 
            The limbs crossing the edges of the table prevented the algorithms from recognizing the table as a rectangle. I decided that this was enough of an issue to pivot and determine a new method for detecting the table. My new idea for table detection is placing 4 colored stickers in the corners. Every several seconds, I will find the location of those colored circles, which will update the location of the walls in the software. This should work even if one of the stickers is hidden by some object. 
            The reason that I want to update the location of the table every several seconds is because of the chance that a high-energy player might bump the table and shift the perspective of the camera. Because I am currently developing an algorithm with the use of a pre-recorded video, I do not have access to the physical table and cannot implement this sticker idea. Instead, I have hard-coded the corners of the table and marked them with blue dots in the video. This is demonstrated below. <br><br>

            <img src="Team\progress\colli384_files\cornerDots.png" alt="Blue Dots Marking Corners" width="530" height="310"><br>
            <i>Blue Dots Marking Corners</i><br><br>
            With these dots in place, I could start determining the approximate position of the walls on the table.<br><br>

            <b>Motion Prediction</b><br>
            Once the wall locations were determined, I started working on how to provide a graphical representation of the predicted movement of the puck. I started by making use of the pseudo-velocity vector that I created last week. This vector used the displacement between the previous frame and current frame to 
            create an estimation on the position in the next frame. Using this vector, I placed a line on top of the puck to indicate the direction it was headed and the magnitude of the predicted displacement. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Velo Vector.mp4" type="video/mp4"></video><br>
            <i>Velocity Vector Overlay</i><br><br>

            This line would help me further understand how the images are being processed by giving me a visual representation of where the puck might be going. Next, I wanted to figure out how to predict when and where a puck might strike a wall. 
            To do this, I made use of the velocity vector as well as the wall locations. I am currently using a coordinate system derived from the pixel locations according to OpenCV. This means that left to right is the positive x-direction and top to bottom is the positive y-direction. 
            I created a class for the puck that contained information about slope from the velocity and some intercept from the puck location. Then, I made 4 of the same objects for each of the walls. For example, the top wall had a slope of 0 and an intercept determined from the location of the table in the video. 
            Now that I had an equation for the puck line that was constantly being updated, and 4 equations for the 4 lines that the walls formed, I could start predicting intersection points. This was done using simple algebra. I set the equations equal to each other and marked the points where the puck line 
            intersected with the wall lines. However, this did not provide much information because all 5 lines were infinite, so unless they were parallel, they would intersect at some point no matter what. In order to combat this, I determined 4 different movement cases: (+x, +y), (+x, -y), (-x, +y), and (-x, -y). 
            If the puck was moving in the positive x-direction and the positive y-direction, I really only needed to worry about intersections with the top wall and the right wall. After narrowing the choices down to two walls, I determined that the best way to predict which of the two walls the puck would collide with was by 
            using the distance equation to compare the distance from the puck to each of the two wall intersection points. Once a single point was determined, I marked that location with a red dot using OpenCV. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Wall Location.mp4" type="video/mp4"></video><br>
            <i>Wall Bounce Predictions</i><br><br>

            This week, I learned that we will need some way to track the location of the table within the field of view of the camera. I believe the method of placing colored stickers in the corner will suffice, but I will not be sure until I can test with the actual table. 
            As far as next steps go, I would like to begin working on predictions for what might happen after collision with a wall. The route I would like to pursue is measuring the angle that the puck strikes the wall, and then using the same angle as the angle of reflection coming off the wall. 
            After calculating that angle, I would like to draw out the path that it might take for a visual representation. I believe I am getting close to being able to determine the final location of the puck. The goal is to implement the aforementioned features by the end of next week. <br><br>


            <h4>Week 3:</h4>
            <b>Date:</b> 09/08/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 24<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, the focus remained on the computer vision software. I worked on fine tuning the tracks to ensure that the puck is the only object being tracked, and it is being tracked with high accuracy. I also began implementing frame by frame predictions on future puck location.
            <br><br>
            <b>Object Detection vs Object Tracking</b><br>
            A large chunk of my time went into changing the methods and parameters of the tracking software. I started by researching object detection vs object tracking.
            At the surface level, it appears that object tracking should be more efficient than object detection. According to <a href="https://learnopencv.com/the-complete-guide-to-object-tracking-in-computer-vision/#:~:text=Usually%20tracking%20algorithms%20are%20faster,and%20speed%20of%20its%20motion."> The Complete Guide to Object Tracking</a>, this is due to the fact that you can use the previous frames of the video to extract information about the location and direction of objects. 
            I agree that this would be true for applications with multiple objects in motion and at relatively low speeds. However, I found that using purely object detection resulted in faster results than object tracking for the air hockey application. 
            I tested this by comparing color based detection to color based detection with a CSRT (Discriminative Correlation Filter with Channel and Spatial Reliability) tracker. With the CRST tracker, I got a playback speed that was slower than the original video speed. This means that the processing was occurring slower than the frames were coming in. In a live video, this would create delays that would not be acceptable for an air hockey application. Shown below is a video of the software that uses the CSRT tracker. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team/progress/colli384_files/CSRT Tracking.mp4" type="video/mp4"> </video><br>
            <i>Puck Tracking with CSRT</i>
            <br><br>

            Due to this discovery, I shifted my focus to completely object detection based methods. I found that because the puck moves so fast and is often occluded by human limbs and goals, tracking would not necessarily be reasonable. We do not care if it is the same puck as before, we just want to find the only moving puck.<br><br>

            <b>Track Tuning</b><br>
            In order to more accurately track the puck, I decided to search through the contours being created by the color mask, and choose the one with the largest area to be highlighted. This would filter out any miscellaneous bounding boxes for things like mallets. 
            I have spoken with the team about acquiring green pucks with low potential for glare for the purposes of color filtering. I found that any varying light reflecting off of the puck can have a significant impact on the perceived color. As a result, we will be looking for matte pucks for the final design. Having one track being followed at all times also allows for ease of coordinate determination. Shown below is a video of the speed at which purely color filtering can be done on a pre-recorded video.<br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Color Tracking.mp4" type="video/mp4"> </video><br>
            <i>Puck Tracking with Color Filtering</i>
            <br><br>

            <b>Frame by Frame Coordinate Prediction</b><br>
            I began working on the framework for what will be the future puck location algorithm. To start off, I used the moments created by the single largest contour to find the centroid of the object. 
            Ideally, this would be the center of the puck every time. Occasionally, the largest contour will be one of the mallets, but that should not be a problem once we switch to a green puck. With this centroid, 
            I was able to use the numpy image pixel array as my coordinate system and locate the puck at any given frame. To determine the "velocity" of the puck, which in this case is just the distance and direction per frame, I kept a history of the previous frame's centroid. 
            I found the difference between new location and old location, and used that value to predict where the puck will be in the next frame. Obviously, this falls apart once the puck collides with a wall. This is a problem that I plan to address next week. To test this algorithm, 
            I printed out the velocity vector of the puck, the predicted location for the next frame, and the actual location in the next frame. Shown below is a video of this process.<br><br>

            <video autoplay muted loop width="550" height="310"> <source src="Team\progress\colli384_files\Location Prediction.mp4" type="video/mp4"> </video><br>
            <i>Frame by Frame Puck Location Prediction</i>
            <br><br>

            As far as results go, I am fairly happy with the progress that I have made with the detections and basic trajectory prediction. I think that these steps will serve as a good foundation for what the project will become. I learned that using CSRT may not be the correct course of action for our purposes and that it is much quicker to just look for a certain color in a frame. 
            Looking forward, I want to start developing a more advanced algorithm for future puck location predictions that take into account the possibility of wall collisions. We also received our camera this week, so I would like to start working with live video as soon as possible. <br><br>
    

            <h4>Weeks 1-2:</h4>
            <b>Date:</b> 09/01/2023<br>
            <b>Total Hours:</b> 16<br>
            <b>Cumulative Hours:</b> 16<br>
            <b>Description of Project Design efforts:</b><br>
            I dedicated the first two weeks of my time to my research goals. As the software engineer for this team, I decided that a solid majority of my time would be best spent learning the basics of computer vision to create a strong foundation for our hockey puck tracking software. The rest of my time was spent with miscellaneous endeavors.
            <br><br>
            <b>Computer Vision Research</b><br>
            As stated previously, the majority of my time went into learning the basics of computer vision, as I have not had any experience with the topic. Abby and I will be doing most of the work on the detection and tracking software, so we decided to begin prototyping using the OpenCV library in Python.
            I started off by watching 2 tutorials on object detection and tracking. The first is called <a href="https://youtu.be/O3b8lVF93jU?si=zo2YENBCpAHVSnqZ"> Object Tracking with Opencv and Python</a>. The second is called <a href="https://youtu.be/GgGro5IV-cs?si=gAnJfifMMWW31Tew"> Object Tracking from scratch with OpenCV and Python</a>. Both were made by a channel called Pysource.
            These two videos provided good information to begin writing my own versions of object detection. I first began by creating a sample video using the videotestsrc element of gstreamer. It was meant to replicate a puck bouncing back and forth around a table. The code I wrote to detect the puck filtered out stagnant objects, and followed dynamic objects using a technique that creates a "mask". Below is a screenshot of this tracking. <br><br>

            <img src="Team/progress/colli384_files/figure1.png" alt="Object Detection with Oversimplified Video" width="390" height="338"> <br>
            <i>Object Detection with Oversimplified Video</i> <br><br>
            
            The next step I took was trying to do object detection on a real-world example. Abby shared the video of two people playing <a href="https://youtu.be/e_cz4DaimyM?si=zmTch2UO63U3RgJN"> mini air hockey</a>, which was quite helpful for a test example.
            The code that I used for the gstreamer example did a horrible job of tracking the puck because it was poorly written and could not handle multiple moving objects at one time. In order to fix this, I had to change the thresholds for a moving object to be classified as an object of interest.
            The contours on the mask each hold an area value that indicates how many pixels the contour takes up on the screen. I increased the lower limit for the area to classify an object as important. Originally, I had it at 100 pixels, but I increased it to anything between 3000 and 5000 pixels, which did a fairly nice job of filtering out any miscellaneous movement, such as hands. Below is an image demonstrating the accuracy of the previously mentioned method. <br><br>

            <img src="Team/progress/colli384_files/figure2.png" alt="Imperfect Object Detection with Real Video" width="529" height="309"> <br>
            <i>Imperfect Object Detection with Real Video</i> <br><br>

            Because motion alone was not enough to distinguish the puck from other objects, I decided to filter out objects by color. To do this, I had to figure out what the hsv range of the puck was. I was not familiar with hsv previously, so I had to look into that. It is similar to an rgb color scheme, but hsv stands for hue, saturation, and value. Once I found the color of the puck through some experimentation, I filtered out any colors that did not fit in that range. I combined the color mask and the motion mask using a bitwise and so that the only things left in the video were moving objects in that color spectrum.
            This technique turned out to be very successful relative to previous attempts. An image of this technique is shown below. <br><br>

            <img src="Team/progress/colli384_files/figure3.png" alt="Object Detection with Motion and Color Mask" width="529" height="309"> <br>
            <i>Object Detection with Motion and Color Mask</i> <br><br>
            
            Abby has been working on shape-based filtering, which could work well with color filtering. I believe that if we choose a puck color that is drastically different from the table and paddles, it would be quite simple to locate the puck. It may turn out in the future that we have to remove the motion mask because of the case where the puck sits still in the middle of the table. The software would not detect an object because it would not be moving. The next step in my research will be keeping track of a specific object, which in this case will be the puck.
            Once we obtain our air hockey table and camera, I would also like to start doing some testing using a live video feed, as opposed to pre-recorded video.
            I think I made good progress on learning OpenCV basics during weeks 1 and 2, and think that I am where I should be with respect to the completion of PSDR 5.
            <br><br>


            <br>    
        </div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
