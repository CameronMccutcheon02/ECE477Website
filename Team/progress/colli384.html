<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
This is an example weekly progress report document that team members can use to report their individual progress 
of their ECE477 senior design projects. Weekly progress reports are expected to follow the general guidelines
presented in the "Progress Report Policy" document, posted on Brightspace.  

Please create 4 copies of this example, renaming each copy to <PurdueID>.html, where <PurdueID> corresponds to
the Purdue ITAP Career Account ID given by Purdue to each individual team member. If you have any questions,
contact course staff.
-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<!--Reconfigurable base tag; used to modify the site root location for root-relative links-->
<!--<base href="https://engineering.purdue.edu/ece477/StudentWebTemplate/" />-->
    <base href="https://engineering.purdue.edu/477grp16/" /> <!-- Replace the N with your team number-->
<!--Content-->
<title>ECE477 Course Documents</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="George Hadley">
<meta name = "format-detection" content = "telephone=no" />
<meta name="viewport" content="width=device-width,initial-scale=1.0">

<!--CSS-->
<link rel="stylesheet" href="css/default.css" type="text/css" media="all" />
<link rel="stylesheet" href="css/responsive.css">
<link rel="stylesheet" href="css/styles.css">
<link rel="stylesheet" href="css/content.css">
<!--[if IE 6]>
<link href="default_ie6.css" rel="stylesheet" type="text/css" />
<![endif]-->

</head>
<body>
<div id="wrapper_site">
    <div id="wrapper_page">
	<!-- Instantiate global site header.-->
	<div id="header"></div>
		<!-- Instantiate site global navigation bar.-->
		<div id="menu"></div>
	
		<!-- Instantiate a page banner image. Page banner images should be 1100x350px and should be located within the local
			img folder located at this directory level. -->
		<div id="banner">
			<img src="Files/img/BannerImgExample.jpg"></img>
		</div>
	
		<!-- Instantiate "tools" needed for a page. Tools are premade functional blocks that can be used to build a page,
			and include things such as a file lister (for listing out homework assignments or tutorials)
		-->
		<div id="content">
            <h2>Progress Report for Joey Collins</h2>

            <h4>Weeks 8-9:</h4>
            <b>Date:</b> 10/20/2023<br>
            <b>Total Hours:</b> 14<br>
            <b>Cumulative Hours:</b> 70<br>
            <b>Description of Project Design Efforts:</b><br>
            In week 8, a significant amount of time was dedicated to the Midterm Design Review Presentation. In week 9, I focused on working more on software. Specifically, I worked on some serial communication foundations with Abby and I developed a way to calibrate the color range for the 
            puck in the scenario that the lighting or puck color changes.<br><br>

            <b>Midterm Design Review Presentation</b><br>
            As stated previously, the majority of week 8 was dedicated to the creation and fine-tuning of the presentation. I was in charge of the software development status section and the
            project timeline section. For the software development status section, I created two charts to demonstrate the individual components of the software. The first chart covered the software that will be running on the PC. This included sub-components for visual analysis and data transfer. The other chart 
            was dedicated to the software that will be running on the microcontroller. This chart included sub-components for output control, PC interfacing, and sensor interfacing. I also took the liberty of including videos to give a visual of the computer vision software that I have been working on. I think this helped give the reviewers a better understanding 
            of what I was explaining. The other section that I covered was the project timeline information. This was not a very difficult task, as everyone is quite aware of the strict timeline that we are currently working with. I decided to use a Gantt chart to represent the components that we are working on and when we hope to have them completed. Overall, the presentation went fairly well. I think that we struggled 
            a little bit with timing, but I am not too upset with that as we had a lot of information to cover with a project this involved.<br><br>

            <b>Puck Dependent Motor Manipulation</b><br>
            One of the more exciting components of the project that I worked on in week 9 dealt with controlling the motors based on information coming from the computer vision software. To do this, 
            I worked closely with Abby and her data serialization software. This software is able to take data and convert it into a format that can be sent and received via UART. This is how we will be sending the data from the PC to the microcontroller. 
            In order to begin working on this basic integration, I altered the software to provide a displacement. Every time the motor is requested to be at a new location, that location is cached. Upon the next request, the distance between the new request and the previous request is calculated and returned. This is the displacement that is being used in this example. 
            Once a displacement is found, the data is serialized and sent to the microcontroller. To check to make sure the data can be translated into instructions for the motors, Abby and I decided to manipulate the direction of the motor spin based upon the sign of the displacement. For example, 
            a positive displacement would result in clockwise spin, and a negative displacement would result in counter-clockwise spin. We did not work with any varying speeds or steps because at this point we only wanted to ensure that we were able to get data from the PC to the motors with no problems. Shown below is a video demonstrating this testing.<br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\ahaluska_files\IMG_1442.mp4" type="video/mp4"></video><br>
            <i>Motor Control using Puck Data</i><br><br>

            One of the disappointing parts of this testing was finding that the motor could not switch direction as frequently as the data was being sent. We were working with an older motor because our new motors were in the machine shop at that time. The hope is that the new motors are able to handle these frequencies better. If not, I will have to work more on smoothing out the detections 
            and being more careful with how often I update the preferred mallet location. Going forward, I will keep this in mind as I continue writing code for the motors.<br><br>

            <b>Color Calibration</b><br>
            The other problem that I concentrated on this week was finding a way to determine the puck color without hard coding it in prior to runtime. This was necessary because of the fact that 
            lighting plays such a large role in how the camera picks up the color of the puck. If the table were to be in a different location than the lab, the range for green that we had established would be completely useless. For example, 
            when the table is transported to MSEE for the SPARK challenge, the lighting would be completely different and the puck would most likely not be found in the range that had been determined previously. To combat this, 
            I have written a script that locates the center of the table (where the puck should be placed during initialization) and calculates an appropriate HSV color range for the color that appears in the middle of the table. I originally thought that I may have to implement some sort of color correction like the method found on <a href="https://pyimagesearch.com/2021/02/15/automatic-color-correction-with-opencv-and-python/">pyimagesearch</a>, but I did not have a persistent need for color correction. I 
            just needed a one-time calculation for a color range.<br><br>
            Looking forward, I would like to continue fine-tuning the data that we are sending out so hopefully we can send data less frequently. This will need to be done in conjunction with the implementation of mallet tracking as well, which I hope to start very soon.



            <br><br>

            <h4>Week 7:</h4>
            <b>Date:</b> 10/06/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 56<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I made the switch to live video analysis. Most of my time was spent fixing bugs that manifested due to the switch between the pre-recorded video and live video.<br><br>

            <b>Color Matching</b><br>
            One of the seemingly simple tasks this week ended up being on of the more time consuming tasks for me. In the software that I had written for the pre-recorded video, the puck is found using color detection. I had to set a range of color in the Hue, Saturation, Value (HSV) scale that the puck could be found in. This was quite easy because the puck 
            was a vibrant red color and the lighting in the video was fairly constant so it did not change appearance much. However, after making the switch to live video I found that finding the range for our green puck to be quite difficult. Part of the problem came from my lack of understanding of how the HSV color scale actually works, so some of the testing was just guessing 
            and checking. I also tried making use of a <a href="https://redketchup.io/color-picker">color picker</a> online. However, this did not work as well as I would have like as the software was still losing the puck quite frequently. I used the value that I got from the color picking website as a starting point and continued my guess and check method. It took several hours of testing until I was able to find a range that 
            consistently matched the puck. Shown below is an image of the green puck being detected in the lab. <br><br>

            <img src="Team\progress\colli384_files\live pic.png" width="530" height="310"><br>
            <i>Green Puck Detection</i><br><br>

            <b>Adjustments for Live Video</b><br>
            After I was able to find the correct color range to find the puck, I decided to run my software that I developed for the pre-recorded video without changes on the live feed. Before I could do this, I had to mount the camera to the top of the table. To do this, I reattached the scoreboard that came with the table. This scoreboard conveniently sits right above the center of 
            the table, so I was able to leave it there and connect the camera to it using zip ties. Unfortunately, I have the camera 90 degrees from the position it needs to be in to view the whole table, so I was working with a weird angle this week. Once the camera was in place, I went ahead and ran my software with the new color range. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\initial live.mp4" type="video/mp4"></video><br>
            <i>Live Video with Initial Parameters</i><br>

            I knew prior to this initial testing that the output would be far from what we wanted. The main problem that I needed to account for was the position of the walls within the video. Like I have said previously, I want to put certain colored stickers on the corners of the table that will indicate 
            the positions and update over time in case the table moves between frames. For now, I am hard-coding the position of the walls in the new video. Due to the camera being oriented incorrectly, I also had to improvise and assume that the right side of the frame was the goal, but it was actually just a wall. This is fine for now, but 
            I will have to adjust the camera soon to start working with the real situation. After fixing the wall locations, I started testing again. This time, the intersection locations of the puck seemed to be much more accurate and I was able to actually see the predictions being made, which were correct for the most part. However, it seemed to twitch more than I would 
            like, so I adjusted some of the parameters for the number of frames in the history of previous intersections and the pixel tolerance for notifying about a new instance of the potential for a goal. This seemed to clean up some of the noise. Shown below is some live feed that was captured to demonstrate the software working with the right side of the table acting as the robot goal.<br><br>

            
            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\Live Feed.mp4" type="video/mp4"></video><br>
            <i>Live Video with Updated Parameters</i><br>

            I was quite happy to see that the transition from pre-recorded video to live video was not as horrific as I had thought. There is still a lot of cleaning up that needs to be done to allow for smooth motor movement, but I think the software is on the right track. 
            This week I learned that we will definitely need to spend time perfecting the parameters and probably add some more algorithms that can lessen the amount of times the motors have to move the robot mallet. Looking forward, I would like to get the stickers to mark the end of the table and I would also like to 
            get some kind of sticker that will indicate where exactly the mallet is located on the table so I can start to track both mallet and puck position at the same time. This will be useful because I will be able to send the microcontroller information about exactly the mallet needs to move, not where it needs to move to. This will decrease 
            the amount of processing that the microcontroller needs to do, allowing for lower latency.<br><br>


            <h4>Week 6:</h4>
            <b>Date:</b> 09/29/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 48<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I started making additions to the puck tracking software with the motor application in mind. I wanted to make sure that the motor is not constantly twitching due to non-essential puck data. I also started developing a plan for how the coordinate system 
            on the table should be executed.<br><br>

            <b>Smoothing Predictions</b><br>
            I noticed that the predictions last week were quite accurate, but the fact that the puck coordinates were constantly jumping around would put quite a strain on the motors. I tried to address this issue by implementing some averaging on the locations where the puck might cross into the "danger zone". 
            To do this, a history of previous locations where the puck crossed into that zone is kept. The length of this history can be changed, but for now I have it keeping the 3 most recent intersections. Using this history, I average out the 3 previous intersection points and output one singular point that takes advantage of the previous data. This should 
            provide more accurate predictions because of the fact that it is no longer relying on just two consecutive frames. It should also decrease the likelihood of the motor making a sporadic movement due to some outlying frame. Keeping the motors in mind, I also created a sort of tolerance to ensure that the micro is not being sent useless data. Essentially, 
            the last location sent is compared to the new location waiting to be sent. If the two differ by more than the tolerance amount, then the new location will be sent. Otherwise, it will not be sent and the motor will stay where it is. This is to ensure that the motors are not being sent information that will result in unnecessary movement. I am not totally sure that these additions will be completely necessary in the final product. 
            It could turn out that the motors can handle extra movement completely fine, and it is preferable to have no averaging. I implemented this functionality just so that we have it in case we need it. It also allows for some customization later on. Shown below is a demonstration of the averaging being used with a history of 3 frames and a tolerance of 30 pixels. <br><br>

            <video autoplay muted loop width="530" height="310"><source src="Team\progress\colli384_files\point averaging.mp4" type="video/mp4"></video><br>
            <i>Puck Location Averaging</i><br><br>

            <b>Grid Planning</b><br>
            The next piece that I worked on this week was developing a plan for how the table will be divided into coordinates for the purpose of moving the robot mallet. The ideal solution would be using the pixel values for the coordinate system. 
            I believe that this would not be easy to execute because of the number of pixels in the image, requiring a large amount of precision in the motor movement. Instead, I think it would be beneficial to scale the pixel values down into smaller values. For example, the top of the table in 
            the pre-recorded video is at pixel value 90. The bottom of the table is at pixel value 625. If we let the top of the table be 0 and let the bottom of the table be 25, then we can use the equation new y-coordinate = 0.0467(old y-coordinate) - 4.21, then we can translate the pixel value into a coordinate system that goes from 0-15. The same can be done for the x-direction. 
            I think that using a coordinate system like this would also decrease the amount of twitching in the motors. For example, a puck prediction that varies by several pixels might cause a jerk in the motor, but several pixels translated to a grid system with fewer discrete points would not make a difference and the motor would not necessarily have to move. 
            I think that a grid with more than 0-15 would be necessary, but that was an easy example to use. This week I learned that it might be more difficult than I thought it would be 
            to optimize the puck detection process. I may have to spend many hours in the future doing some calibration once we move to a live feed.<br><br>
            
            I did not do any testing with live video this week because work is still being done on the table for the mechanical components. I hope that next week, we will be able to mount the camera at the top of the table to start testing live video. If this is not possible, 
            I plan to begin developing a plan on the exact data to send to the microcontroller to move the motors. Unfortunately, this will require knowledge of where the robot mallet is located. In live feed, I was planning on doing this using color differentiation, but in the pre-recorded video the puck and mallets are the same color. I would have to write the code without being able to test it.<br><br>


            <h4>Week 5:</h4>
            <b>Date:</b> 09/22/2023<br>
            <b>Total Hours:</b> 9<br>
            <b>Cumulative Hours:</b> 40<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I continued working on the tracking software. The components I worked on this week were the ability to predict future bounces on the table and a system to detect when and where the puck will potentially cross some threshold that indicates potential for a goal.<br><br>
            <b>Future Bounce Prediction</b><br>
            As of last week, the tracking software was able to determine where a puck might hit a wall based on its current trajectory. This week, I wanted to expand on that capability by predicting where the puck might go after that bounce. 
            In order to realize this goal, I used some knowledge of geometry. Shown below is the formula for finding the <a href="https://www.onlinemathlearning.com/anglebetweentwolines.html">angle of intersection</a> between two infinite lines using slope.<br><br>
            <img src="Team\progress\colli384_files\angle.png" width="220" height="115"><br>
            <i>Angle of Intersection Formula</i><br><br>
            
            After observing this formula, it can be shown that a line intersecting another line with an undefined or 0 slope will result in a reflection with the negative of the original slope. 
            Once this was determined, I could use the same function I had been using for bounce detection, but this time with the new reflection line. I would also have to update the velocity so the function knew which way the puck was traveling along that line. This was simple to do because if the puck bounces off of the top or bottom walls, then the x component stays the same and the y component is negated. The opposite is true for bounces off of the left or right walls. 
            I found that I could recursively use this function to predict as many future bounces as I want. I have the number currently capped at 3 for observational purposes. This seems like a good number to use for testing as well because pucks appear to slow down significantly after 3 bounces. 
            Shown below is video demonstrating the bounce prediction functionality. The three red dots indicate the 3 places where the puck will strike if no other external forces act upon it.<br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\future bounce.mp4" type="video/mp4"></video><br>
            <i>Future Bounce Prediction</i><br><br>
            <b>Goal Potential Detection</b><br>
            The next component that I worked on was trying to determine when the puck has a chance of coming near the robot's goal. The current method that I have for doing this is establishing a line segment slightly in front of the physical goal that represents a "danger zone". 
            Currently, this line segment goes from the top of the table to the bottom of the table and rests about 100 pixels in front of the goal. I edited the function that predicts future bounces to include the software that checks if the current path or any reflections will cross that line into the "danger zone". 
            It also checks to make sure the puck is moving towards the robot goal before indicating that there is potential for a goal. In the future, I think this will be an important function to prevent the motors from having to twitch at every movement of the puck and instead wait until there is an apparent scoring chance. 
            Shown below is video demonstrating this functionality. Any time there is a red line segment with a green dot at the end of it, the puck is predicted to cross into this "danger zone" within 3 collisions of its current path. The red line segment shows what bounce the scoring chance will come from.<br><br>
            
            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\danger zone.mp4" type="video/mp4"></video><br>
            <i>Goal Scoring Potential Prediction</i><br><br>

            I think that good progress was made this week in terms of tracking software development. I would really like to begin working with live video as soon as we can get the camera mounted above our table. I believe that my current code can be easily edited to theoretically work with live video, 
            but sometimes things do not work that way. Next week, I hope to begin optimizing my current code so the predictions do not jump around as much. I want to do this because I want to prevent unnecessary motor movement.<br><br>


            <h4>Week 4:</h4>
            <b>Date:</b> 09/15/2023<br>
            <b>Total Hours:</b> 7<br>
            <b>Cumulative Hours:</b> 31<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, I started working on how the tracking software should be making predictions on final puck location. Specifically, I worked on handling collisions with walls. Unfortunately because of Industrial Roundtable, I was not able to progress as far as I wanted to this week.
            <br><br>
            <b>Table Detector</b><br>
            In order to determine when a puck is going to collide with a wall, the software needs to know where the walls are located. To do this, I want to locate the corners and determine the edges using those points. My initial idea for doing this was using the OpenCV object detector. I wrote some code to detect any objects with 4 edges in the video, but occlusion became quite a significant problem in this method. 
            The limbs crossing the edges of the table prevented the algorithms from recognizing the table as a rectangle. I decided that this was enough of an issue to pivot and determine a new method for detecting the table. My new idea for table detection is placing 4 colored stickers in the corners. Every several seconds, I will find the location of those colored circles, which will update the location of the walls in the software. This should work even if one of the stickers is hidden by some object. 
            The reason that I want to update the location of the table every several seconds is because of the chance that a high-energy player might bump the table and shift the perspective of the camera. Because I am currently developing an algorithm with the use of a pre-recorded video, I do not have access to the physical table and cannot implement this sticker idea. Instead, I have hard-coded the corners of the table and marked them with blue dots in the video. This is demonstrated below. <br><br>

            <img src="Team\progress\colli384_files\cornerDots.png" alt="Blue Dots Marking Corners" width="530" height="310"><br>
            <i>Blue Dots Marking Corners</i><br><br>
            With these dots in place, I could start determining the approximate position of the walls on the table.<br><br>

            <b>Motion Prediction</b><br>
            Once the wall locations were determined, I started working on how to provide a graphical representation of the predicted movement of the puck. I started by making use of the pseudo-velocity vector that I created last week. This vector used the displacement between the previous frame and current frame to 
            create an estimation on the position in the next frame. Using this vector, I placed a line on top of the puck to indicate the direction it was headed and the magnitude of the predicted displacement. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Velo Vector.mp4" type="video/mp4"></video><br>
            <i>Velocity Vector Overlay</i><br><br>

            This line would help me further understand how the images are being processed by giving me a visual representation of where the puck might be going. Next, I wanted to figure out how to predict when and where a puck might strike a wall. 
            To do this, I made use of the velocity vector as well as the wall locations. I am currently using a coordinate system derived from the pixel locations according to OpenCV. This means that left to right is the positive x-direction and top to bottom is the positive y-direction. 
            I created a class for the puck that contained information about slope from the velocity and some intercept from the puck location. Then, I made 4 of the same objects for each of the walls. For example, the top wall had a slope of 0 and an intercept determined from the location of the table in the video. 
            Now that I had an equation for the puck line that was constantly being updated, and 4 equations for the 4 lines that the walls formed, I could start predicting intersection points. This was done using simple algebra. I set the equations equal to each other and marked the points where the puck line 
            intersected with the wall lines. However, this did not provide much information because all 5 lines were infinite, so unless they were parallel, they would intersect at some point no matter what. In order to combat this, I determined 4 different movement cases: (+x, +y), (+x, -y), (-x, +y), and (-x, -y). 
            If the puck was moving in the positive x-direction and the positive y-direction, I really only needed to worry about intersections with the top wall and the right wall. After narrowing the choices down to two walls, I determined that the best way to predict which of the two walls the puck would collide with was by 
            using the distance equation to compare the distance from the puck to each of the two wall intersection points. Once a single point was determined, I marked that location with a red dot using OpenCV. This is shown below. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Wall Location.mp4" type="video/mp4"></video><br>
            <i>Wall Bounce Predictions</i><br><br>

            This week, I learned that we will need some way to track the location of the table within the field of view of the camera. I believe the method of placing colored stickers in the corner will suffice, but I will not be sure until I can test with the actual table. 
            As far as next steps go, I would like to begin working on predictions for what might happen after collision with a wall. The route I would like to pursue is measuring the angle that the puck strikes the wall, and then using the same angle as the angle of reflection coming off the wall. 
            After calculating that angle, I would like to draw out the path that it might take for a visual representation. I believe I am getting close to being able to determine the final location of the puck. The goal is to implement the aforementioned features by the end of next week. <br><br>


            <h4>Week 3:</h4>
            <b>Date:</b> 09/08/2023<br>
            <b>Total Hours:</b> 8<br>
            <b>Cumulative Hours:</b> 24<br>
            <b>Description of Project Design Efforts:</b><br>
            This week, the focus remained on the computer vision software. I worked on fine tuning the tracks to ensure that the puck is the only object being tracked, and it is being tracked with high accuracy. I also began implementing frame by frame predictions on future puck location.
            <br><br>
            <b>Object Detection vs Object Tracking</b><br>
            A large chunk of my time went into changing the methods and parameters of the tracking software. I started by researching object detection vs object tracking.
            At the surface level, it appears that object tracking should be more efficient than object detection. According to <a href="https://learnopencv.com/the-complete-guide-to-object-tracking-in-computer-vision/#:~:text=Usually%20tracking%20algorithms%20are%20faster,and%20speed%20of%20its%20motion."> The Complete Guide to Object Tracking</a>, this is due to the fact that you can use the previous frames of the video to extract information about the location and direction of objects. 
            I agree that this would be true for applications with multiple objects in motion and at relatively low speeds. However, I found that using purely object detection resulted in faster results than object tracking for the air hockey application. 
            I tested this by comparing color based detection to color based detection with a CSRT (Discriminative Correlation Filter with Channel and Spatial Reliability) tracker. With the CRST tracker, I got a playback speed that was slower than the original video speed. This means that the processing was occurring slower than the frames were coming in. In a live video, this would create delays that would not be acceptable for an air hockey application. Shown below is a video of the software that uses the CSRT tracker. <br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team/progress/colli384_files/CSRT Tracking.mp4" type="video/mp4"> </video><br>
            <i>Puck Tracking with CSRT</i>
            <br><br>

            Due to this discovery, I shifted my focus to completely object detection based methods. I found that because the puck moves so fast and is often occluded by human limbs and goals, tracking would not necessarily be reasonable. We do not care if it is the same puck as before, we just want to find the only moving puck.<br><br>

            <b>Track Tuning</b><br>
            In order to more accurately track the puck, I decided to search through the contours being created by the color mask, and choose the one with the largest area to be highlighted. This would filter out any miscellaneous bounding boxes for things like mallets. 
            I have spoken with the team about acquiring green pucks with low potential for glare for the purposes of color filtering. I found that any varying light reflecting off of the puck can have a significant impact on the perceived color. As a result, we will be looking for matte pucks for the final design. Having one track being followed at all times also allows for ease of coordinate determination. Shown below is a video of the speed at which purely color filtering can be done on a pre-recorded video.<br><br>

            <video autoplay muted loop width="530" height="310"> <source src="Team\progress\colli384_files\Color Tracking.mp4" type="video/mp4"> </video><br>
            <i>Puck Tracking with Color Filtering</i>
            <br><br>

            <b>Frame by Frame Coordinate Prediction</b><br>
            I began working on the framework for what will be the future puck location algorithm. To start off, I used the moments created by the single largest contour to find the centroid of the object. 
            Ideally, this would be the center of the puck every time. Occasionally, the largest contour will be one of the mallets, but that should not be a problem once we switch to a green puck. With this centroid, 
            I was able to use the numpy image pixel array as my coordinate system and locate the puck at any given frame. To determine the "velocity" of the puck, which in this case is just the distance and direction per frame, I kept a history of the previous frame's centroid. 
            I found the difference between new location and old location, and used that value to predict where the puck will be in the next frame. Obviously, this falls apart once the puck collides with a wall. This is a problem that I plan to address next week. To test this algorithm, 
            I printed out the velocity vector of the puck, the predicted location for the next frame, and the actual location in the next frame. Shown below is a video of this process.<br><br>

            <video autoplay muted loop width="550" height="310"> <source src="Team\progress\colli384_files\Location Prediction.mp4" type="video/mp4"> </video><br>
            <i>Frame by Frame Puck Location Prediction</i>
            <br><br>

            As far as results go, I am fairly happy with the progress that I have made with the detections and basic trajectory prediction. I think that these steps will serve as a good foundation for what the project will become. I learned that using CSRT may not be the correct course of action for our purposes and that it is much quicker to just look for a certain color in a frame. 
            Looking forward, I want to start developing a more advanced algorithm for future puck location predictions that take into account the possibility of wall collisions. We also received our camera this week, so I would like to start working with live video as soon as possible. <br><br>
    

            <h4>Weeks 1-2:</h4>
            <b>Date:</b> 09/01/2023<br>
            <b>Total Hours:</b> 16<br>
            <b>Cumulative Hours:</b> 16<br>
            <b>Description of Project Design efforts:</b><br>
            I dedicated the first two weeks of my time to my research goals. As the software engineer for this team, I decided that a solid majority of my time would be best spent learning the basics of computer vision to create a strong foundation for our hockey puck tracking software. The rest of my time was spent with miscellaneous endeavors.
            <br><br>
            <b>Computer Vision Research</b><br>
            As stated previously, the majority of my time went into learning the basics of computer vision, as I have not had any experience with the topic. Abby and I will be doing most of the work on the detection and tracking software, so we decided to begin prototyping using the OpenCV library in Python.
            I started off by watching 2 tutorials on object detection and tracking. The first is called <a href="https://youtu.be/O3b8lVF93jU?si=zo2YENBCpAHVSnqZ"> Object Tracking with Opencv and Python</a>. The second is called <a href="https://youtu.be/GgGro5IV-cs?si=gAnJfifMMWW31Tew"> Object Tracking from scratch with OpenCV and Python</a>. Both were made by a channel called Pysource.
            These two videos provided good information to begin writing my own versions of object detection. I first began by creating a sample video using the videotestsrc element of gstreamer. It was meant to replicate a puck bouncing back and forth around a table. The code I wrote to detect the puck filtered out stagnant objects, and followed dynamic objects using a technique that creates a "mask". Below is a screenshot of this tracking. <br><br>

            <img src="Team/progress/colli384_files/figure1.png" alt="Object Detection with Oversimplified Video" width="390" height="338"> <br>
            <i>Object Detection with Oversimplified Video</i> <br><br>
            
            The next step I took was trying to do object detection on a real-world example. Abby shared the video of two people playing <a href="https://youtu.be/e_cz4DaimyM?si=zmTch2UO63U3RgJN"> mini air hockey</a>, which was quite helpful for a test example.
            The code that I used for the gstreamer example did a horrible job of tracking the puck because it was poorly written and could not handle multiple moving objects at one time. In order to fix this, I had to change the thresholds for a moving object to be classified as an object of interest.
            The contours on the mask each hold an area value that indicates how many pixels the contour takes up on the screen. I increased the lower limit for the area to classify an object as important. Originally, I had it at 100 pixels, but I increased it to anything between 3000 and 5000 pixels, which did a fairly nice job of filtering out any miscellaneous movement, such as hands. Below is an image demonstrating the accuracy of the previously mentioned method. <br><br>

            <img src="Team/progress/colli384_files/figure2.png" alt="Imperfect Object Detection with Real Video" width="529" height="309"> <br>
            <i>Imperfect Object Detection with Real Video</i> <br><br>

            Because motion alone was not enough to distinguish the puck from other objects, I decided to filter out objects by color. To do this, I had to figure out what the hsv range of the puck was. I was not familiar with hsv previously, so I had to look into that. It is similar to an rgb color scheme, but hsv stands for hue, saturation, and value. Once I found the color of the puck through some experimentation, I filtered out any colors that did not fit in that range. I combined the color mask and the motion mask using a bitwise and so that the only things left in the video were moving objects in that color spectrum.
            This technique turned out to be very successful relative to previous attempts. An image of this technique is shown below. <br><br>

            <img src="Team/progress/colli384_files/figure3.png" alt="Object Detection with Motion and Color Mask" width="529" height="309"> <br>
            <i>Object Detection with Motion and Color Mask</i> <br><br>
            
            Abby has been working on shape-based filtering, which could work well with color filtering. I believe that if we choose a puck color that is drastically different from the table and paddles, it would be quite simple to locate the puck. It may turn out in the future that we have to remove the motion mask because of the case where the puck sits still in the middle of the table. The software would not detect an object because it would not be moving. The next step in my research will be keeping track of a specific object, which in this case will be the puck.
            Once we obtain our air hockey table and camera, I would also like to start doing some testing using a live video feed, as opposed to pre-recorded video.
            I think I made good progress on learning OpenCV basics during weeks 1 and 2, and think that I am where I should be with respect to the completion of PSDR 5.
            <br><br>


            <br>    
        </div>
	
		<!-- Instantiate global footer. Any changes to the footer should be made through the top-level file "footer.html" -->
		<div id="footer"></div>
    </div>
</div>

<!--JS-->
<script src="js/jquery.js"></script>
<script src="js/jquery-migrate-1.1.1.js"></script>

<script type="text/javascript">
$(document).ready(function() {
    $("#header").load("header.html");
	$("#menu").load("navbar.html");
	$("#footer").load("footer.html");
});
</script>
</body>
</html>
